{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e558b9e1-ee1c-466a-bfb2-807b3c2cabf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "np.set_printoptions(suppress=True)\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18b1087b-c10a-4d72-ab45-20d5c3905be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: mps\n"
     ]
    }
   ],
   "source": [
    "# attempt to autodetect device\n",
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "print(f\"using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a7e986a-5031-4627-a1f4-0f3aba83bd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom dataset for Tiny Shakespeare\n",
    "class TinyShakespeareDataset(Dataset):\n",
    "    def __init__(self, text, seq_length):\n",
    "        self.text = text\n",
    "        self.seq_length = seq_length\n",
    "        self.chars = sorted(list(set(text)))\n",
    "        self.char_to_idx = {ch: i for i, ch in enumerate(self.chars)}\n",
    "        self.idx_to_char = {i: ch for i, ch in enumerate(self.chars)}\n",
    "        self.data_size = len(self.text)\n",
    "\n",
    "    def __len__(self):\n",
    "        return max(0, self.data_size - self.seq_length)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = [self.char_to_idx[c] for c in self.text[index:index+self.seq_length]]\n",
    "        y = [self.char_to_idx[c] for c in self.text[index+1:index+self.seq_length+1]]\n",
    "        return torch.tensor(x).float(), torch.tensor(y).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66e9d29f-9a3e-4703-85e6-a9fa8bc5f508",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "# Download Tiny Shakespeare dataset\n",
    "def download_tiny_shakespeare():\n",
    "    url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "    if not os.path.exists(\"data/tinyshakespeare.txt\"):\n",
    "        data = requests.get(url).text\n",
    "        with open(\"data/tinyshakespeare.txt\", \"w\") as f:\n",
    "            f.write(data)\n",
    "        \n",
    "# Download the dataset\n",
    "download_tiny_shakespeare()\n",
    "\n",
    "# Read the dataset\n",
    "with open(\"data/tinyshakespeare.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Set parameters\n",
    "seq_length = 25\n",
    "BS = 128\n",
    "\n",
    "# Create dataset and split into train and test\n",
    "dataset = TinyShakespeareDataset(text, seq_length)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Create data loaders\n",
    "loaders = {\n",
    "    'train': DataLoader(train_dataset, batch_size=BS, shuffle=True),\n",
    "    'test': DataLoader(test_dataset, batch_size=BS, shuffle=True),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05661f32-101d-493a-8c8b-010598fbadde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNModel(\n",
       "  (i2h): Linear(in_features=1, out_features=64, bias=False)\n",
       "  (h2h): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (h2o): Linear(in_features=64, out_features=65, bias=True)\n",
       "  (sm): LogSoftmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_size = 1\n",
    "hidden_size = 64\n",
    "output_size = len(dataset.chars)\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.i2h = nn.Linear(input_size, hidden_size, bias=False)\n",
    "        self.h2h = nn.Linear(hidden_size, hidden_size)\n",
    "        self.h2o = nn.Linear(hidden_size, output_size)\n",
    "        self.sm = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, x, hidden_state):\n",
    "        x = F.relu(self.i2h(x))\n",
    "        hidden_state = self.h2h(x)\n",
    "        x = F.relu(x + hidden_state)\n",
    "        x = F.relu(self.h2o(x))\n",
    "        x = self.sm(x)\n",
    "        return x, hidden_state\n",
    "model = RNNModel(input_size, hidden_size, output_size)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87551de2-1579-4745-b281-a63bae466659",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model: RNNModel, dataset: TinyShakespeareDataset, prediction_length: int = 100) -> str:\n",
    "    \"\"\"\n",
    "    Generate text up to prediction_length characters\n",
    "    This function requires the dataset as argument in order to properly\n",
    "    generate the text and return the output as strings\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    predicted = dataset.vector_to_string([random.randint(0, len(dataset.chars) -1)])\n",
    "    hidden = model.init_zero_hidden()\n",
    "\n",
    "    for i in range(prediction_length - 1):\n",
    "        last_char = torch.Tensor([dataset.char_to_idx[predicted[-1]]])\n",
    "        X, hidden = last_char.to(device), hidden.to(device)\n",
    "        out, hidden = model(X, hidden)\n",
    "        result = torch.multinomial(nn.functional.softmax(out, 1), 1).item()\n",
    "        #result = out.argmax().item()\n",
    "        predicted += dataset.idx_to_char[result]\n",
    "\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f57e019f-534e-43dd-976c-819b4245e042",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optim = torch.optim.Adam(model.parameters(), lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f8c49d2-015c-49a5-a3cb-c40c753bf70d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 101.4089\n",
      "Loss: 100.6088\n",
      "Loss: 100.5733\n",
      "Loss: 100.4023\n",
      "Loss: 100.4382\n",
      "Loss: 100.2468\n",
      "Loss: 100.0467\n",
      "Loss: 100.5013\n",
      "Loss: 100.4897\n",
      "Loss: 100.7325\n",
      "Loss: 100.3112\n",
      "Loss: 100.6046\n",
      "Loss: 99.9849\n",
      "Loss: 100.0906\n",
      "Loss: 100.6104\n",
      "Loss: 100.2124\n",
      "Loss: 100.1337\n",
      "Loss: 99.4178\n",
      "Loss: 99.7654\n",
      "Loss: 99.6583\n",
      "Loss: 99.4897\n",
      "Loss: 100.1628\n",
      "Loss: 100.0264\n",
      "Loss: 99.8076\n",
      "Loss: 100.1403\n",
      "Loss: 100.0202\n",
      "Loss: 99.8405\n",
      "Loss: 100.2561\n",
      "Loss: 99.9041\n",
      "Loss: 99.7877\n",
      "Loss: 100.0734\n",
      "Loss: 100.2575\n",
      "Loss: 99.7063\n",
      "Loss: 99.8013\n",
      "Loss: 99.0286\n",
      "Loss: 99.7730\n",
      "Loss: 100.5199\n",
      "Loss: 99.9648\n",
      "Loss: 99.4465\n",
      "Loss: 99.8179\n",
      "Loss: 99.7983\n",
      "Loss: 99.8722\n",
      "Loss: 100.4260\n",
      "Loss: 99.3983\n",
      "Loss: 99.8932\n",
      "Loss: 99.8282\n",
      "Loss: 99.7684\n",
      "Loss: 99.7113\n",
      "Loss: 99.3331\n",
      "Loss: 99.8361\n",
      "Loss: 100.3058\n",
      "Loss: 99.7410\n",
      "Loss: 99.3862\n",
      "Loss: 99.2665\n",
      "Loss: 99.8048\n",
      "Loss: 100.1091\n",
      "Loss: 99.9150\n",
      "Loss: 99.2351\n",
      "Loss: 99.7043\n",
      "Loss: 99.7076\n",
      "Loss: 99.9636\n",
      "Loss: 99.3525\n",
      "Loss: 100.0504\n",
      "Loss: 100.0443\n",
      "Loss: 99.6168\n",
      "Loss: 100.1837\n",
      "Loss: 99.8117\n",
      "Loss: 100.1734\n",
      "Loss: 100.1300\n",
      "Loss: 99.7815\n",
      "Loss: 99.9692\n",
      "Loss: 99.7560\n",
      "Loss: 99.7253\n",
      "Loss: 99.6844\n",
      "Loss: 100.1145\n",
      "Loss: 99.7787\n",
      "Loss: 99.6455\n",
      "Loss: 100.0100\n",
      "Loss: 99.9964\n",
      "Loss: 100.2943\n",
      "Loss: 99.0768\n",
      "Loss: 100.0621\n",
      "Loss: 99.0333\n",
      "Loss: 99.4889\n",
      "Loss: 99.2763\n",
      "Loss: 98.8864\n",
      "Loss: 99.0867\n",
      "Loss: 99.0714\n",
      "Loss: 99.4667\n",
      "Loss: 99.1685\n",
      "Loss: 98.7680\n",
      "Loss: 99.2799\n",
      "Loss: 99.3571\n",
      "Loss: 99.2471\n",
      "Loss: 99.0653\n",
      "Loss: 99.2201\n",
      "Loss: 99.1891\n",
      "Loss: 98.8220\n",
      "Loss: 99.6528\n",
      "Loss: 99.2825\n",
      "Loss: 98.9703\n",
      "Loss: 99.1902\n",
      "Loss: 99.5714\n",
      "Loss: 99.2773\n",
      "Loss: 98.4762\n",
      "Loss: 98.5116\n",
      "Loss: 98.6407\n",
      "Loss: 98.4440\n",
      "Loss: 98.5871\n",
      "Loss: 99.1246\n",
      "Loss: 98.6782\n",
      "Loss: 91.8945\n",
      "Loss: 91.8856\n",
      "Loss: 91.8887\n",
      "Loss: 91.6279\n",
      "Loss: 91.4234\n",
      "Loss: 91.3010\n",
      "Loss: 91.7700\n",
      "Loss: 90.9294\n",
      "Loss: 90.6736\n",
      "Loss: 90.7929\n",
      "Loss: 90.7022\n",
      "Loss: 91.0122\n",
      "Loss: 90.3721\n",
      "Loss: 90.4428\n",
      "Loss: 90.5744\n",
      "Loss: 91.1805\n",
      "Loss: 91.3112\n",
      "Loss: 90.2849\n",
      "Loss: 90.0700\n",
      "Loss: 91.0871\n",
      "Loss: 90.4502\n",
      "Loss: 91.0265\n",
      "Loss: 90.6484\n",
      "Loss: 90.8563\n",
      "Loss: 90.7968\n",
      "Loss: 91.0579\n",
      "Loss: 90.3847\n",
      "Loss: 90.1710\n",
      "=> epoch: 1, loss: 3.958245038986206\n",
      "Loss: 90.6878\n",
      "Loss: 90.3832\n",
      "Loss: 89.4205\n",
      "Loss: 91.1549\n",
      "Loss: 90.9836\n",
      "Loss: 90.2969\n",
      "Loss: 90.5136\n",
      "Loss: 91.3997\n",
      "Loss: 90.0124\n",
      "Loss: 90.6685\n",
      "Loss: 90.2301\n",
      "Loss: 89.9751\n",
      "Loss: 90.6314\n",
      "Loss: 90.4525\n",
      "Loss: 91.4299\n",
      "Loss: 90.9553\n",
      "Loss: 90.4165\n",
      "Loss: 90.7461\n",
      "Loss: 89.9480\n",
      "Loss: 89.7277\n",
      "Loss: 90.7935\n",
      "Loss: 90.7900\n",
      "Loss: 91.0992\n",
      "Loss: 90.9664\n",
      "Loss: 90.6697\n",
      "Loss: 90.5240\n",
      "Loss: 88.9293\n",
      "Loss: 91.0237\n",
      "Loss: 90.0660\n",
      "Loss: 89.8496\n",
      "Loss: 90.0993\n",
      "Loss: 90.4571\n",
      "Loss: 91.3002\n",
      "Loss: 90.3134\n",
      "Loss: 90.1432\n",
      "Loss: 89.7176\n",
      "Loss: 89.6179\n",
      "Loss: 90.3059\n",
      "Loss: 91.2095\n",
      "Loss: 89.6610\n",
      "Loss: 90.1415\n",
      "Loss: 90.6413\n",
      "Loss: 89.2091\n",
      "Loss: 90.0702\n",
      "Loss: 91.3199\n",
      "Loss: 91.3778\n",
      "Loss: 89.9580\n",
      "Loss: 90.2149\n",
      "Loss: 89.9304\n",
      "Loss: 89.7574\n",
      "Loss: 89.7043\n",
      "Loss: 90.4557\n",
      "Loss: 89.7722\n",
      "Loss: 90.0946\n",
      "Loss: 90.2822\n",
      "Loss: 90.5955\n",
      "Loss: 90.2976\n",
      "Loss: 91.9524\n",
      "Loss: 90.4066\n",
      "Loss: 90.6772\n",
      "Loss: 90.6663\n",
      "Loss: 90.2655\n",
      "Loss: 90.4442\n",
      "Loss: 89.8523\n",
      "Loss: 90.9462\n",
      "Loss: 90.4999\n",
      "Loss: 89.6144\n",
      "Loss: 91.1817\n",
      "Loss: 91.0122\n",
      "Loss: 90.6744\n",
      "Loss: 90.1839\n",
      "Loss: 90.9518\n",
      "Loss: 89.6514\n",
      "Loss: 89.3132\n",
      "Loss: 89.8391\n",
      "Loss: 89.8055\n",
      "Loss: 90.4947\n",
      "Loss: 90.8997\n",
      "Loss: 89.6610\n",
      "Loss: 90.3109\n",
      "Loss: 89.9014\n",
      "Loss: 90.9623\n",
      "Loss: 89.5329\n",
      "Loss: 91.0991\n",
      "Loss: 91.1486\n",
      "Loss: 90.6566\n",
      "Loss: 90.4764\n",
      "Loss: 91.2651\n",
      "Loss: 90.2293\n",
      "Loss: 90.4872\n",
      "Loss: 89.4195\n",
      "Loss: 90.7367\n",
      "Loss: 90.7654\n",
      "Loss: 90.6384\n",
      "Loss: 90.5891\n",
      "Loss: 90.7806\n",
      "Loss: 89.3500\n",
      "Loss: 90.9282\n",
      "Loss: 89.8788\n",
      "Loss: 91.0678\n",
      "Loss: 90.7358\n",
      "Loss: 91.1299\n",
      "Loss: 90.7745\n",
      "Loss: 89.7643\n",
      "Loss: 90.2999\n",
      "Loss: 90.0254\n",
      "Loss: 89.8453\n",
      "Loss: 90.5587\n",
      "Loss: 89.3200\n",
      "Loss: 90.2353\n",
      "Loss: 91.0809\n",
      "Loss: 89.5849\n",
      "Loss: 92.1764\n",
      "Loss: 89.0080\n",
      "Loss: 90.3094\n",
      "Loss: 90.3604\n",
      "Loss: 90.5973\n",
      "Loss: 89.9488\n",
      "Loss: 91.1108\n",
      "Loss: 90.8647\n",
      "Loss: 89.7011\n",
      "Loss: 91.0795\n",
      "Loss: 90.9989\n",
      "Loss: 90.7133\n",
      "Loss: 91.2430\n",
      "Loss: 90.1099\n",
      "Loss: 90.0014\n",
      "Loss: 89.8954\n",
      "Loss: 90.4756\n",
      "Loss: 89.5284\n",
      "Loss: 90.6907\n",
      "Loss: 90.2505\n",
      "Loss: 90.2253\n",
      "Loss: 90.2065\n",
      "Loss: 91.7226\n",
      "Loss: 90.2774\n",
      "Loss: 89.6195\n",
      "Loss: 90.6834\n",
      "Loss: 89.9059\n",
      "=> epoch: 2, loss: 3.617067337036133\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 2\n",
    "train_losses = {}\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_losses = list()\n",
    "    for i, (X, Y) in enumerate(loaders['train']):\n",
    "        if X.shape[0] != BS:\n",
    "            continue\n",
    "        hidden = torch.zeros(hidden_size, hidden_size, requires_grad=False).float()\n",
    "        optim.zero_grad()\n",
    "        X, Y, hidden = X.to(device), Y.to(device), hidden.to(device)\n",
    "        loss = 0\n",
    "        for c in range(X.shape[1]):\n",
    "            out, hidden = model(X[:, c].reshape(X.shape[0],1), hidden)\n",
    "            l = criterion(out, Y[:, c].long())\n",
    "            loss += l\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 3)\n",
    "        optim.step()\n",
    "\n",
    "        epoch_losses.append(loss.detach().item() / X.shape[1])\n",
    "        if (i+1) % 50 == 0:\n",
    "            print('Loss: {:.4f}'.format(loss.detach().item()))\n",
    "    train_losses[epoch] = torch.tensor(epoch_losses).mean()\n",
    "    print(f'=> epoch: {epoch + 1}, loss: {train_losses[epoch]}')\n",
    "    #print(generate_text(model, data.dataset))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
